{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example inference pipeline\n",
    "This notebook infers lineages from the [Briney et al. 2019](https://doi.org/10.1038/s41586-019-0879-y) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download data  \n",
    "Annotated data can be downloaded using links provided in the [briney/grp_paper repository](https://github.com/briney/grp_paper).  \n",
    "The procedure below takes all replicates together and introduces AIRR-compatible format requred by HILARy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://burtonlab.s3.amazonaws.com/sequencing-data/hiseq_2016-supplement/316188_HNCHNBCXY_consensus_UID18-cdr3nt-90_minimal_071817.tar.gz\n",
    "!tar -xvf 316188_HNCHNBCXY_consensus_UID18-cdr3nt-90_minimal_071817.tar.gz --directory data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Compatible\n",
    "compatible = Compatible()\n",
    "usecols = ['seq_id',\n",
    "           'chain',\n",
    "           'productive',\n",
    "           'v_full',\n",
    "           'j_full',\n",
    "           'cdr3_nt',\n",
    "           'v_start',\n",
    "           'vdj_nt',\n",
    "           'isotype']\n",
    "dirname = 'data/consensus-cdr3nt-90_minimal'\n",
    "dfs = []\n",
    "for filename in tqdm(os.listdir(dirname)):\n",
    "    df = pd.read_csv(os.path.join(dirname,filename),usecols=usecols)\n",
    "    dfs.append(compatible.df2airr(df))\n",
    "df = pd.concat(dfs,ignore_index=True)\n",
    "df['sequence_id'] = df.index\n",
    "filename = 'data/316188_ids.tsv.gz'\n",
    "df[['seq_id','sequence_id']].to_csv(filename,sep='\\t',index=False)\n",
    "df.drop('seq_id',axis=1,inplace=True)\n",
    "filename = 'data/316188.tsv.gz'\n",
    "usecols = ['sequence_id',\n",
    "           'v_call',\n",
    "           'j_call',\n",
    "           'junction',\n",
    "           'v_sequence_alignment',\n",
    "           'j_sequence_alignment',\n",
    "           'v_germline_alignment',\n",
    "           'j_germline_alignment']\n",
    "df[usecols].to_csv(filename,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HILARy pre-inference  \n",
    "Partition the dataset into VJl classes.  \n",
    "For larfest classes, compute distributions of pairwise distances.  \n",
    "Use a priori estimates of prevalence to define high-precision and high-sensitivity thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['sequence_id',\n",
    "           'v_call',\n",
    "           'j_call',\n",
    "           'junction',\n",
    "           'v_sequence_alignment',\n",
    "           'j_sequence_alignment',\n",
    "           'v_germline_alignment',\n",
    "           'j_germline_alignment']\n",
    "filename = 'data/316188.tsv.gz'\n",
    "df = pd.read_table(filename,usecols=usecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apriori import preprocess\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apriori import Apriori\n",
    "ap = Apriori(df)\n",
    "ap.get_histograms(df.loc[ap.productive])\n",
    "ap.get_parameters()\n",
    "ap.get_thresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HILARy inference  \n",
    "Define high-precision and high-sensitivity partition  \n",
    "Apply the full method to high-sensitivity classes that require further partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import HILARy\n",
    "from inference import CDR3Clustering\n",
    "hilary = HILARy(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = CDR3Clustering(ap.classes[hilary.group+['precise_threshold']])\n",
    "sens = CDR3Clustering(ap.classes[hilary.group+['sensitive_threshold']])\n",
    "df['precise_cluster'] = prec.infer(df.loc[ap.productive])\n",
    "df['sensitive_cluster'] = sens.infer(df.loc[ap.productive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hilary.to_do(df)\n",
    "df['family'] = hilary.infer(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HILARy",
   "language": "python",
   "name": "hilary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
